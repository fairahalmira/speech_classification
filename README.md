---

# ğŸ§ Klasifikasi Suara Hewan dengan CNN

Proyek ini membangun dan melatih sebuah **Convolutional Neural Network (CNN)** untuk mengklasifikasikan suara hewan yang ditiru manusia ke dalam 5 kategori.
Model dilatih menggunakan fitur **MFCC** yang diekstrak dari rekaman audio, dan diimplementasikan dengan **PyTorch**.

---

## ğŸ¾ Kategori Suara

* ğŸ„ **Sapi** â€” *moo*
* ğŸˆ **Kucing** â€” *meow*
* ğŸ• **Anjing** â€” *woof*
* ğŸ **Kambing** â€” *mbee*
* ğŸ¦ **Burung** â€” *tweet*

---

## ğŸ“ Dataset

Dataset diperoleh dari **rekaman suara manusia** yang meniru suara hewan-hewan di atas.
Setiap kelas memiliki **15 rekaman audio**, masing-masing dengan:

* **10 rekaman** untuk training
* **5 rekaman** untuk testing

Semua rekaman disimpan di dalam folder `./data/` dengan struktur:

```
data/
â”œâ”€â”€ moo/
â”œâ”€â”€ meow/
â”œâ”€â”€ woof/
â”œâ”€â”€ mbee/
â”œâ”€â”€ tweet/
```

Setiap file adalah `.wav` berdurasi 2 detik, disimpan dengan nama sesuai label.

---

## ğŸ”„ Preprocessing Data

- âœ… **Voice Activity Detection:** menghapus bagian diam  
- âœ… **Normalisasi amplitudo:** menyeragamkan skala amplitudo  
- âœ… **Padding/Cropping:** menyeragamkan panjang rekaman  
- âœ… **Ekstraksi fitur MFCC:** menghasilkan 40 koefisien per frame  
- âœ… **Membuat dataset custom:** menyesuaikan format untuk PyTorch  
- âœ… **Membagi data:** menjadi data train dan test
---

## ğŸ—ï¸ Model

Model CNN sederhana dengan arsitektur:

* 2 blok Conv2D + ReLU + MaxPooling
* Flatten layer
* Fully Connected Layer â†’ output 5 kelas
* Loss: CrossEntropyLoss
* Optimizer: Adam
* Epochs: 100

---

## ğŸ“ˆ Hasil Pelatihan

* Model mencapai akurasi **100%** pada dataset kecil ini (train & test).
* Visualisasi loss & akurasi selama training juga disertakan dalam notebook.
* Laporan klasifikasi (precision, recall, f1-score) ditampilkan.

ğŸ“Œ *Catatan:*
Akurasi tinggi ini wajar karena dataset sangat kecil dan tidak bervariasi. Untuk aplikasi nyata, diperlukan dataset yang lebih besar dan beragam.

---

## ğŸš€ Uji Coba Model

Setelah dilatih, model dapat digunakan untuk mengklasifikasikan **rekaman suara baru** secara langsung:

* Merekam audio dengan mikrofon.
* Mengekstrak fitur MFCC.
* Mengeluarkan prediksi label dengan confidence untuk masing-masing kelas.

Contoh output:

```
moo: 0.00%
meow: 0.00%
woof: 99.87%
mbee: 0.00%
tweet: 0.13%

Prediksi: woof
```

---

## ğŸ“¦ File Penting

```
.
â”œâ”€â”€ vocal.ipynb              # Notebook berisi seluruh proses
â”œâ”€â”€ data/                    # Folder dataset audio
â”œâ”€â”€ vowel_cnn.pth            # Model terlatih (disimpan)
â”œâ”€â”€ vowel_formants_comparison.png  # Visualisasi formant
â”œâ”€â”€ README.md                # Penjelasan proyek ini
```

---

## ğŸ“ Cara Menjalankan

### Persyaratan

* Python 3.13.3
* PyTorch
* librosa
* sounddevice
* soundfile
* matplotlib
* scikit-learn
* Jupyter Notebook

### Langkah

1ï¸âƒ£ Clone repo & masuk ke folder:

```bash
git clone <link-repo>
cd speech_classification
```

2ï¸âƒ£ Install dependencies:

```bash
pip install -r requirements.txt
```

3ï¸âƒ£ Jalankan notebook:

```bash
jupyter notebook vocal.ipynb
```

4ï¸âƒ£ Ikuti langkah-langkah di dalam notebook untuk melatih & menguji model.

---

## ğŸ“š Referensi

* [PyTorch documentation](https://pytorch.org/docs/stable/index.html)
* [Librosa documentation](https://librosa.org/doc/latest/index.html)
* [Formant frequency reference](https://biomed.papers.upol.cz/pdfs/bio/2007/02/31.pdf)

---

## ğŸ‘¨â€ğŸ’» Kontributor

* Nama: \[FairahAlmira]
* Univ: \[Institut Teknologi Padang]
* Program: \[AI Development @ Infinite Learning]

---

